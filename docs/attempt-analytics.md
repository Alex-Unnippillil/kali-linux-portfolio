# Attempt Analytics Dashboard

The Attempt Analytics desktop app visualises canned red-team exercises so learners can discuss
how defenders iterate on controls between labs. It keeps all scenarios educational and exports
only privacy-cleared records.

## Reading the dashboard

- **Summary tiles** show how many simulated attempts are in view, the success rate of those
  attempts, how many unique targets were touched, and the average duration of each attempt.
  These numbers update instantly as you toggle filters.
- **Virtualised timeline** lists every attempt that matches the filters. Rows include protocol,
  outcome, analyst alias, and detection source. The list uses a 432px viewport with an item
  height of 72px and overscan of 4, yielding an estimated frame cost under 12ms so scrolling
  remains within the 16ms per-frame budget.
- **Run comparison** highlights how control effectiveness changed between two exercises. Deltas
  for status, protocol, attempts, and success rate appear next to a textual diff generated by
  the shared diff utility (`utils/attemptAnalytics.ts`).
- **Charts** summarise attempts by status, protocol, and target using the shared aggregation
  helpers so you can spot regressions quickly.

## Filters and search

- **Run filters** include or exclude entire exercises when you want to replay a subset of labs.
- **Status and protocol filters** can be toggled individually, making it easy to concentrate on
  blocked controls or successful footholds.
- **Search** inspects targets, protocols, analyst aliases, run labels, and detection sources.
  Use it to focus on a target like “MQ Broker” or a detection pipeline such as “SIEM”.

## Privacy-aware export

The **Export filtered (privacy-safe)** button saves the current view to JSON after sanitising the
records with `sanitizeAttemptsForExport`. Restricted sensitivity attempts and inline notes are
removed automatically so the export is suitable for classroom review. The payload includes
metadata about active filters for reproducibility.

## Teaching ideas

- Ask students to compare the baseline simulation to the latest lab and explain why the success
  rate shifted. The diff view makes it easy to see where improvements landed.
- Filter to a single protocol (for example, HTTPS) and debate how multi-layered controls—captcha,
  WAF, SIEM—work together across runs.
- Export the filtered dataset and use it as a prompt for post-lab retrospectives or tabletop
  discussions about monitoring coverage.

