# Experimentation Guide

Use this guide whenever you propose, run, or close an experiment in the Kali Linux Portfolio. It complements the issue template in `.github/ISSUE_TEMPLATE/experiment.yml` and keeps experiment hygiene consistent across contributors.

## 1. Before you launch

1. **Validate the hypothesis.** Confirm that the experiment has a clear hypothesis tied to a product outcome or learning goal.
2. **Confirm the minimum sample size.** Experiments should collect *at least 500 qualified sessions* (or the statistically equivalent user count) before you evaluate the primary metric. Use your analytics platform’s calculator if you expect high variance and document the expected volume in the issue template.
3. **Set the runtime window.** Plan for a *minimum duration of 7 days* to capture weekday/weekend behavior unless the population is extremely high-volume.
4. **Check guardrail metrics.** Identify the non-regression metrics (error rate, retention, latency, etc.) you will track in addition to the success metric.
5. **Instrument monitoring.** Create or link a dashboard that reports the primary metric and guardrails daily. Set up real-time alerts for severe regressions (>5% drop in guardrails or >3σ anomalies).
6. **Secure approvals.** Share the filled issue template with product, design, and engineering stakeholders. They should explicitly approve the sample size, monitoring plan, and launch window before the feature flag flips.

## 2. During the experiment

- **Daily monitoring cadence.** Owners must review the dashboard once per business day. Post a short update in the experiment issue summarizing the latest numbers and any anomalies.
- **Review checkpoints.** The issue template includes an explicit list of checkpoints. Honor those dates and tag the assigned reviewers.
- **Alert response.** If guardrail alerts trigger, pause or roll back the experiment immediately and document the action taken in the issue.
- **Automated reminders.** A scheduled workflow (`experiment-digest.yml`) posts a dashboard summary in the Actions run log and pings experiments that have not been updated in seven days. Use those reminders to stay ahead of drift.

## 3. Wrapping up

1. **Reach the required volume.** Do not conclude early unless a guardrail breach forces a rollback.
2. **Document results.** Summarize the final metrics, decision (ship, iterate, or stop), and supporting evidence directly in the issue template sections.
3. **Apply the cleanup plan.** Remove temporary assets (feature flags, experiment buckets, mock data) and update public-facing documentation as promised in the issue.
4. **Archive dashboards.** Snapshot the monitoring dashboard and link it in the issue before removing temporary analytics views.
5. **Retrospective.** Add key learnings to the team’s knowledge base (e.g., `docs/reviews/`), especially if the experiment informs future roadmap items.

## 4. Stakeholder communication and feedback

- **Weekly digest.** Every Monday the experiment owners share a link to the latest Actions run summary (`Experiment Digest`) in the stakeholder channel or mailing list. Copy the markdown generated by the workflow for quick dissemination.
- **Standing review.** Hold a 15-minute review in the weekly product sync to walk through active experiments, next checkpoints, and any blockers.
- **Feedback loop.** Invite stakeholders to add comments directly in the experiment issue or open follow-up issues tagged with `feedback`. Capture agreed-upon changes to guardrails or process tweaks in this guide.
- **Process changes.** When you adjust the guardrails or workflow, update this document and notify stakeholders during the next review so the team can acknowledge the change.

## 5. Quick reference

- **Plan the experiment:** Open a new issue using the *Experiment plan* template.
- **Verify guardrails:** Minimum 500 qualified sessions, 7-day baseline runtime, daily dashboard checks.
- **Monitor automatically:** Review the output from `.github/workflows/experiment-digest.yml` for reminders and stale experiment flags.
- **Close the loop:** Apply the cleanup plan, record learnings, and gather stakeholder feedback.
